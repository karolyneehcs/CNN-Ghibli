{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29069cc8-52ef-4eec-bfa2-9f77fbb741b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - torch\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-arm64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-arm64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install matplotlib --yes\n",
    "!conda install torchvision --yes\n",
    "!conda install torch --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34602afb-f953-4e31-92b7-d4c91f69c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy\n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7830c3-f087-4952-bc1f-469001aaafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observando disponibilidade do cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a04d9e-8148-4e2d-86a5-3c66e7c46fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a720aca1-164f-4990-b619-da2235943c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual network from convolutional neural networks \n",
    "\n",
    "class ResBlock(nn.Module): \n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        #defining 3 convolutional layers 2D \n",
    "        self.conv1 = nn.Conv2d(in_channels,16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16,32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        #defining average pooling (or maxpool) for the layer\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        #defining the layer 2D\n",
    "        self.convOut = nn.Conv2d(64,out_channels, kernel_size=1)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.pool1(x)\n",
    "        x = self.convOut(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407da6ca-e195-4466-886c-f5c9c750875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 512, 8) <class 'numpy.ndarray'>\n",
      "torch.Size([1, 16, 256, 4])\n"
     ]
    }
   ],
   "source": [
    "#script phase \n",
    "\n",
    "block = ResBlock(3, 16)\n",
    "X = np.random.randn(3, 512, 8)\n",
    "X = np.float32(X)\n",
    "print(X.shape, type(X))\n",
    "\n",
    "x = torch.from_numpy(X)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "out = block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fc3a83-0f4e-4b7b-9096-3d3d9f514825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to the image output \n",
    "#TODO: as imagens podem sair 1024 ou 512 pq temos gpu \n",
    "def ImageSeg(image):\n",
    "    imageSize = 1024 if torch.cuda.is_available() else 512\n",
    "\n",
    "    loader = transforms.Compose([\n",
    "    transforms.Resize(imageSize), \n",
    "    transforms.ToTensor])\n",
    "\n",
    "    imageLoad = Image.open(image)\n",
    "    imageLoad = loader(imageLoad).unsqueeze(0)\n",
    "    return imageLoad.to(device, torch.float)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd37752-9db4-45c0-acad-b60ff99f8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agora faz o contrario \n",
    "def TensorToImage(tensor, title=None): \n",
    "    unloader = transforms.ToPILImage()\n",
    "    plt.ion()\n",
    "\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    imageRes = unloader(image)\n",
    "\n",
    "    plt.imshow(imageRes)\n",
    "\n",
    "    if title is not None: \n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2b0b51-0630-4df8-9eab-1831cf6a82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function \n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, target): \n",
    "        super(ContentLoss, self).__init__\n",
    "        self.target = target.detach()\n",
    "        \n",
    "    def forward(self, input): \n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd4349b-08d3-4978-b392-224638aba8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gramMatrix(input): \n",
    "        a, b, c, d = input.size()\n",
    "        #a batch size \n",
    "        #b = number of feature maps \n",
    "        #c and d are dimensions of a feature map \n",
    "        features = input.view(a * b, c* d)\n",
    "        \n",
    "        G= torch.mm(features, features.t())\n",
    "        return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7b0be3-4a23-4a80-8977-f4af279d16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module): \n",
    "    \n",
    "    def __init__(self, targetFeature): \n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.targetFeature = gramMatrix(targetFeature).detach()\n",
    "        \n",
    "    def forward(self, input): \n",
    "        G = gramMatrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a99b316-643a-4d03-acdf-72a286a04a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anakarolina/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/anakarolina/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "cnn = models.vgg19(pretrained=True).features.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1683fc2-7cd3-4958-be39-3a95818ddae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "class Normalization(nn.Module): \n",
    "    def __init__(sefl, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b798670-1e75-4b4a-9818-f92d42e2c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def getStyleModelAndLoss(cnn, normalizationMean, normalizationStd, styleImg, contentImg, contentLayers=content_layers_default, styleLayers=style_layers_default):\n",
    "    normalization = Normalization(normalizationMean, normalizationStd)\n",
    "    contentLosses = []\n",
    "    styleLosses = []\n",
    "    \n",
    "    model = nn.Sequential(normalization)\n",
    "    i = 0 \n",
    "    \n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i+=1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'batchn_{}'.format(i)\n",
    "        else: \n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "        \n",
    "    model.add_module(name, layer)\n",
    "    \n",
    "    if name in contentLayers:\n",
    "        target = model(contentImg).detach()\n",
    "        contentLoss = ContentLoss(target)\n",
    "        model.add_module(\"content_loss_{}\".format(i), contentLoss)\n",
    "        contentLosses.append(contentLoss)\n",
    "    \n",
    "    if name in styleLayers: \n",
    "        targetFeature = model(styleImg).detach()\n",
    "        styleLoss = StyleLoss(targetFeature)\n",
    "        model.add_module(\"style_loss_{}\".format(i), styleLoss)\n",
    "        styleLosses.append(styleLoss)\n",
    "        \n",
    "    for i in ragen(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break \n",
    "        model = model[:(i + 1)]\n",
    "        \n",
    "    return model, styleLosses, contentLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7121ab25-1c6b-41dd-93f2-813e2dfcdb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def getStyleModelAndLosses(cnn, normalizationMean, normalizationStd, styleImg, contentImg, contentLayers=content_layers_default, styleLayers=style_layers_default):\n",
    "    normalization = Normalization(normalizationMean, normalizationStd)\n",
    "    contentLosses = []\n",
    "    styleLosses = []\n",
    "    \n",
    "    model = nn.Sequential(normalization)\n",
    "    i = 0 \n",
    "    \n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i+=1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'batchn_{}'.format(i)\n",
    "        else: \n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "        \n",
    "    model.add_module(name, layer)\n",
    "    \n",
    "    if name in contentLayers:\n",
    "        target = model(contentImg).detach()\n",
    "        contentLoss = ContentLoss(target)\n",
    "        model.add_module(\"content_loss_{}\".format(i), contentLoss)\n",
    "        contentLosses.append(contentLoss)\n",
    "    \n",
    "    if name in styleLayers: \n",
    "        targetFeature = model(styleImg).detach()\n",
    "        styleLoss = StyleLoss(targetFeature)\n",
    "        model.add_module(\"style_loss_{}\".format(i), styleLoss)\n",
    "        styleLosses.append(styleLoss)\n",
    "        \n",
    "    for i in ragen(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break \n",
    "        model = model[:(i + 1)]\n",
    "        \n",
    "    return model, styleLosses, contentLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1aa1e49-82d9-4df9-b590-6606f82c81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy of the content image here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31d4d485-b4e0-43ff-a7d1-80ac0bec77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputOptimizer(inputImage):\n",
    "    optimizer = optim.LBFGS([inputImage])\n",
    "    return optimizer\n",
    "\n",
    "def runStyleTranser(cnn, normalizationMean, normalizationStd, contentImg, styleImg, inputImage, numSteps=300, styleWeight=1000000, contentWeight=1):\n",
    "    print(\"building the style transfer\")\n",
    "    model, styleLosses, contentLosses = getStyleModelAndLosses(cnn, normalizationMean, normalizationStd, styleImg, contentImg)\n",
    "    \n",
    "    inputImage.requires_grad(True)\n",
    "    model.eval()\n",
    "    model.requires_grad(False)\n",
    "    \n",
    "    optimizer = getInputOptimizer(inputImage)\n",
    "    print(\"Optimizing...\")\n",
    "    run = [0]\n",
    "    while run[0] <= numSetps: \n",
    "        \n",
    "        def closure(): \n",
    "            with torch.no_grad():\n",
    "                inputImg.clamp_(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            model(inputImage)\n",
    "            styleScore = 0\n",
    "            contentScore = 0\n",
    "            \n",
    "            for sl in styleLosses: \n",
    "                styleScore += sl.loss\n",
    "            for cl in contentLosses: \n",
    "                contentScore += cl.loss\n",
    "                \n",
    "            styleScore *= styleWeight\n",
    "            contentScore *= contentWeight\n",
    "            \n",
    "            loss = styleScore + contentScore\n",
    "            loss.backward()\n",
    "            \n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print('Style Loss: {:4f}'.format(styleScore.item(), contentScore.item()))\n",
    "                print()\n",
    "                \n",
    "            return styleScore + contentScore\n",
    "        optimizer.step(closure)\n",
    "    with torch.no_grad():\n",
    "        inputImage.clamp_(0, 1)\n",
    "    return inputImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fbc802b-363b-4e52-82d7-8f9690a81239",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageStyle = []\n",
    "for filename in glob.glob('Documents/Ghibli(paisagens)/*.png'):\n",
    "    im=Image.open(filename)\n",
    "    plt.figure()\n",
    "    plt.imshow(im) \n",
    "    plt.show(im)\n",
    "    imageForProccess = ImageSeg(im)\n",
    "    imageStyle.append(imageForProccess)\n",
    "\n",
    "#print(imageStyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc139ef-2716-44bc-a865-8616e4725503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
