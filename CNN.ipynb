{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c58db1b0-ecb3-4c47-b8f0-18fff6c824bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - torch\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-arm64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-arm64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install matplotlib --yes\n",
    "!conda install torchvision --yes\n",
    "!conda install torch --yes\n",
    "!conda install Pillow --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34602afb-f953-4e31-92b7-d4c91f69c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "from collections import deque\n",
    "import torch.optim as optim\n",
    "from PIL import Image \n",
    "from multiprocessing import Pool\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy\n",
    "import os \n",
    "from glob import glob\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7830c3-f087-4952-bc1f-469001aaafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observando disponibilidade do cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a04d9e-8148-4e2d-86a5-3c66e7c46fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a720aca1-164f-4990-b619-da2235943c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual network from convolutional neural networks \n",
    "\n",
    "class ResBlock(nn.Module): \n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        #defining 3 convolutional layers 2D \n",
    "        self.conv1 = nn.Conv2d(in_channels,16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16,32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        #defining average pooling (or maxpool) for the layer\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        #defining the layer 2D\n",
    "        self.convOut = nn.Conv2d(64,out_channels, kernel_size=1)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.pool1(x)\n",
    "        x = self.convOut(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407da6ca-e195-4466-886c-f5c9c750875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 512, 8) <class 'numpy.ndarray'>\n",
      "torch.Size([1, 16, 256, 4])\n"
     ]
    }
   ],
   "source": [
    "#script phase \n",
    "\n",
    "block = ResBlock(3, 16)\n",
    "X = np.random.randn(3, 512, 8)\n",
    "X = np.float32(X)\n",
    "print(X.shape, type(X))\n",
    "\n",
    "x = torch.from_numpy(X)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "out = block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fc3a83-0f4e-4b7b-9096-3d3d9f514825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to the image output \n",
    "#TODO: as imagens podem sair 1024 ou 512 pq temos gpu \n",
    "def ImageSeg(image):\n",
    "    imageSize = 1024 if torch.cuda.is_available() else 512\n",
    "\n",
    "    loader = transforms.Compose([\n",
    "    transforms.Resize(imageSize), \n",
    "    transforms.ToTensor()])\n",
    "\n",
    "    imageLoad = Image.open(image)\n",
    "    imageLoad = loader(imageLoad).unsqueeze(0)\n",
    "    return imageLoad.to(device, torch.float)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd37752-9db4-45c0-acad-b60ff99f8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agora faz o contrario \n",
    "def TensorToImage(tensor, title=None): \n",
    "    unloader = transforms.ToPILImage()\n",
    "    plt.ion()\n",
    "\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    imageRes = unloader(image)\n",
    "\n",
    "    plt.imshow(imageRes)\n",
    "\n",
    "    if title is not None: \n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2b0b51-0630-4df8-9eab-1831cf6a82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function \n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, target): \n",
    "        super(ContentLoss, self).__init__\n",
    "        self.target = target.detach()\n",
    "        \n",
    "    def forward(self, input): \n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd4349b-08d3-4978-b392-224638aba8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gramMatrix(input): \n",
    "        a, b, c, d = input.size()\n",
    "        #a batch size \n",
    "        #b = number of feature maps \n",
    "        #c and d are dimensions of a feature map \n",
    "        features = input.view(a * b, c* d)\n",
    "        \n",
    "        G= torch.mm(features, features.t())\n",
    "        return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7b0be3-4a23-4a80-8977-f4af279d16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module): \n",
    "    \n",
    "    def __init__(self, targetFeature): \n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gramMatrix(targetFeature).detach()\n",
    "        \n",
    "    def forward(self, input): \n",
    "        G = gramMatrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1683fc2-7cd3-4958-be39-3a95818ddae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "class Normalization(nn.Module): \n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a99b316-643a-4d03-acdf-72a286a04a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = models.vgg19(weights='DEFAULT').features.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b798670-1e75-4b4a-9818-f92d42e2c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def getStyleModelAndLoss(cnn, normalizationMean, normalizationStd, styleImg, contentImg, contentLayers=content_layers_default, styleLayers=style_layers_default):\n",
    "    normalization = Normalization(normalizationMean, normalizationStd)\n",
    "    contentLosses = []\n",
    "    styleLosses = []\n",
    "    \n",
    "    model = nn.Sequential(normalization)\n",
    "    i = 0 \n",
    "    \n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i+=1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'batchn_{}'.format(i)\n",
    "        else: \n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "        \n",
    "    model.add_module(name, layer)\n",
    "    \n",
    "    if name in contentLayers:\n",
    "        target = model(contentImg).detach()\n",
    "        contentLoss = ContentLoss(target)\n",
    "        model.add_module(\"content_loss_{}\".format(i), contentLoss)\n",
    "        contentLosses.append(contentLoss)\n",
    "    \n",
    "    if name in styleLayers: \n",
    "        targetFeature = model(styleImg).detach()\n",
    "        styleLoss = StyleLoss(targetFeature)\n",
    "        model.add_module(\"style_loss_{}\".format(i), styleLoss)\n",
    "        styleLosses.append(styleLoss)\n",
    "        \n",
    "    for i in ragen(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break \n",
    "        model = model[:(i + 1)]\n",
    "        \n",
    "    return model, styleLosses, contentLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7121ab25-1c6b-41dd-93f2-813e2dfcdb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def getStyleModelAndLosses(cnn, normalizationMean, normalizationStd, styleImg, contentImg, contentLayers=content_layers_default, styleLayers=style_layers_default):\n",
    "    normalization = Normalization(normalizationMean, normalizationStd)\n",
    "    contentLosses = []\n",
    "    styleLosses = []\n",
    "    \n",
    "    model = nn.Sequential(normalization)\n",
    "    i = 0 \n",
    "    \n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i+=1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'batchn_{}'.format(i)\n",
    "        else: \n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "        \n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in contentLayers:\n",
    "            target = model(contentImg).detach()\n",
    "            contentLoss = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(i), contentLoss)\n",
    "            contentLosses.append(contentLoss)\n",
    "\n",
    "        if name in styleLayers: \n",
    "            targetFeature = model(styleImg).detach()\n",
    "            styleLoss = StyleLoss(targetFeature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), styleLoss)\n",
    "            styleLosses.append(styleLoss)\n",
    "        \n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break \n",
    "        model = model[:(i + 1)]\n",
    "        \n",
    "    return model, styleLosses, contentLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1aa1e49-82d9-4df9-b590-6606f82c81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy of the content image here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31d4d485-b4e0-43ff-a7d1-80ac0bec77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputOptimizer(inputImage):\n",
    "    optimizer = optim.LBFGS([inputImage])\n",
    "    return optimizer\n",
    "\n",
    "def runStyleTranser(cnn, normalizationMean, normalizationStd, contentImg, styleImg, inputImage, numSteps=300, styleWeight=1000000, contentWeight=1):\n",
    "    print(\"building the style transfer\")\n",
    "    model, styleLosses, contentLosses = getStyleModelAndLosses(cnn, normalizationMean, normalizationStd, styleImg, contentImg)\n",
    "\n",
    "    inputImage.requires_grad_(True)\n",
    "    model.eval()\n",
    "    model.requires_grad_(False)\n",
    "    \n",
    "    optimizer = getInputOptimizer(inputImage)\n",
    "    print(\"Optimizing...\")\n",
    "    run = [0]\n",
    "    while run[0] <= numSteps: \n",
    "        \n",
    "        def closure(): \n",
    "            with torch.no_grad():\n",
    "                inputImage.clamp_(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            model(inputImage)\n",
    "            styleScore = 0\n",
    "            contentScore = 0\n",
    "            \n",
    "            for sl in styleLosses: \n",
    "                styleScore += sl.loss\n",
    "            for cl in contentLosses: \n",
    "                contentScore += cl.loss\n",
    "                \n",
    "            styleScore *= styleWeight\n",
    "            contentScore *= contentWeight\n",
    "            \n",
    "            loss = styleScore + contentScore\n",
    "            loss.backward()\n",
    "            \n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}:\".format(run[0]))\n",
    "                print('Style Loss: {:4f}'.format(styleScore.item(), contentScore.item()))\n",
    "                print()\n",
    "                    \n",
    "                print(\"salvando imagem\")\n",
    "                outImage = inputImage.clone().detach()\n",
    "                outImage.clamp_(0, 1)\n",
    "                imageName = f\"output_{run[0]}.jpg\"\n",
    "                pathToSave = os.path.join('/resultados/', imageName)\n",
    "                outImage = outImage.squeeze(0).cpu().numpy()\n",
    "                outImage = np.transpose(outImage, (1, 2, 0))\n",
    "                outImage = (outImage * 255).astype(np.uint8)\n",
    "                outImage = Image.fromarray(outImage)\n",
    "                outImage.save(save_path)\n",
    "                print(\"imagem salva com nome {imageName}\")\n",
    "                \n",
    "            return styleScore + contentScore\n",
    "        optimizer.step(closure)\n",
    "    with torch.no_grad():\n",
    "        inputImage.clamp_(0, 1)\n",
    "    return inputImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fbc802b-363b-4e52-82d7-8f9690a81239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcao para carregar imagens de estilo \n",
    "imageStyle = []\n",
    "directory = os.path.dirname(\"Ghibli(paisagens)/\")\n",
    "for image in os.listdir(directory):\n",
    "    if image.endswith(\".png\"):\n",
    "        im = Image.open(os.path.join(directory, image)).convert(\"RGB\")\n",
    "        #print(im.size)\n",
    "        processIm = ImageSeg(os.path.join(directory, image))\n",
    "        imageStyle.append(processIm)\n",
    "\n",
    "# tudo certo até aqui família \n",
    "# agora fazer pra imagem ser aplicada \n",
    "imageContent = []\n",
    "inputImage = []\n",
    "directory2 = os.path.dirname(\"landscapeDef/\")\n",
    "for image in os.listdir(directory2):\n",
    "    if image.endswith(\".jpg\"):\n",
    "        im2 = Image.open(os.path.join(directory2, image))\n",
    "        #print(im2.size)\n",
    "        appImStyle = ImageSeg(os.path.join(directory2, image))\n",
    "        imageContent.append(appImStyle)\n",
    "        \n",
    "        \n",
    "#white noise to the input image \n",
    "inputImage = copy.deepcopy(imageContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74dc3142-5b2c-4f32-aba2-e3a3261f7c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "for elements in imageContent: \n",
    "    print(type(elements))\n",
    "    print(elements.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9905bdf-cf07-412c-a0a8-5cbf7df7dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "inputImage = torch.stack(inputImage).squeeze()\n",
    "#print(type(inputImage))\n",
    "imageStyle = torch.stack(imageStyle).squeeze()\n",
    "imageContent = torch.stack(imageContent).squeeze()\n",
    "\n",
    "print(type(imageContent))\n",
    "print(type(imageStyle))\n",
    "print(type(inputImage))\n",
    "\n",
    "inputImage2 = inputImage.clone().detach()\n",
    "imageStyle2 = imageStyle.clone().detach()\n",
    "imageContent2 = imageContent.clone().detach()\n",
    "\n",
    "#imageContent = torch.cat(imageContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960b13e-a45b-4660-9965-2c9b83426f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building the style transfer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rh/rl36xhr16gjf12xfxckxhb3c0000gn/T/ipykernel_40088/2923708280.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
      "/var/folders/rh/rl36xhr16gjf12xfxckxhb3c0000gn/T/ipykernel_40088/2923708280.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std = torch.tensor(std).view(-1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "output = runStyleTranser(cnn, cnn_normalization_mean, cnn_normalization_std, imageContent, imageStyle, inputImage)\n",
    "plt.figure()\n",
    "imshow(output, title=\"Output Image\")\n",
    "plt.ioff()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a87f7-eb83-4c9f-ae4a-748a5ce1ad4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
